{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33c34b89-4b09-4e70-b0d6-a2b5d26a5b9e",
   "metadata": {},
   "source": [
    "# Exercise 2 - Image Classification with ML and DL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6c2542-fdfe-4a4d-8881-3b17b21c89c3",
   "metadata": {},
   "source": [
    "In this exercise you and your team have to finish the implementation of the following machine learning code.  \n",
    "\n",
    "This exercise consists of the following steps which have to work in conjunction:\n",
    "* Data visualization - Visualize the data to become familiar with it and identify possible patterns and problems\n",
    "* Data preparation - Prepare the data for it to be usable later in the machine learning model\n",
    "* Model creation - Chose/build a machine learning model suitable for the given task\n",
    "* Model training - Train your model with the data you prepared\n",
    "* Model evaluation - Evaluate the performance of your model. For this choose appropriate measurement metrics and visualize it with the help of graphs and figures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99675558",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ad1237",
   "metadata": {},
   "source": [
    "**You will find tasks all throughout this notebook. The start and the end of a task is marked through parting lines as in between these lines you can add as many code cells as you need to finish the task and to add your documentation.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e94502f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12dd9c9f-5d39-455a-b9ad-0bb4ab6bdba6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Part 1 - Classical Machine Learning Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312e257b-61e3-4dca-b79a-366efda192d7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Important imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0341d3-c6e1-4f5c-a29c-565ef65c0b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import os\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b05040f-3ef8-49fe-a818-8c99d38773e0",
   "metadata": {},
   "source": [
    "### Inspect the data \n",
    "\n",
    "The first step of every machine learning project is to make oneself familiar with the data.  \n",
    "\n",
    "* In what form is the data accessible?\n",
    "* How many samples are there?\n",
    "* Are they already sorted by class?\n",
    "* etc.\n",
    "\n",
    "It is important to look through them programmatically and manually to see if maybe some prior preparation is needed before even loading the data.\n",
    "\n",
    "You can use the following code to check, if your dataset is in the right place (data/CIFAR-10-images/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e27337-9d35-45fe-befb-467b3d4fd017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get information about the folder contents\n",
    "for dirpath, dirnames, filenames in os.walk(\"data/CIFAR-10-images/\"):\n",
    "    print(f\"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecc3913",
   "metadata": {},
   "source": [
    "**Since each sample is already sorted into its own dictionary we don't have to do it ourselves and can take the folder names as names for our classes we want our model to be able to classify**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0a4bd4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f588beb6-702a-4e85-9e64-faf1304d6f1c",
   "metadata": {},
   "source": [
    "#### Task 01 - Generate Class Names\n",
    "\n",
    "##### Use the Numpy *np.array()* to create a list of class names from the subdirectories!\n",
    "\n",
    "Take a look here if you're stuck: https://numpy.org/doc/stable/\n",
    "\n",
    "The output should look something like this:\n",
    "\n",
    "`['airplane' 'automobile' 'bird' 'cat' 'deer' 'dog' 'frog' 'horse' 'ship'\n",
    " 'truck'] 10`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28eacc07-c11e-41ca-8eeb-d2ebc423b125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get class names from folders\n",
    "data_dir = pathlib.Path(\"data/CIFAR-10-images/train/\")  # turn our training path into a Python path\n",
    "\n",
    "class_names =  # TODO: create a list of class_names from the subdirectories\n",
    "\n",
    "print(class_names, len(class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b3d368-ced6-4f7a-b408-2b140bf28824",
   "metadata": {},
   "source": [
    "#### Next, you need to associate class names with numbers.\n",
    "\n",
    "The output should look something like this:\n",
    "`(10,\n",
    " {'airplane': 0,\n",
    "  'automobile': 1,\n",
    "  'bird': 2,\n",
    "  'cat': 3,\n",
    "  'deer': 4,\n",
    "  'dog': 5,\n",
    "  'frog': 6,\n",
    "  'horse': 7,\n",
    "  'ship': 8,\n",
    "  'truck': 9})`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e23adfb-4411-49e2-90ac-3fcaca0cb5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# associate class names with numbers, machinelaerning and deeplearning works with numbers!\n",
    "class_names_dict =  # TODO: create the dictionary\n",
    "len(class_names_dict), class_names_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c4ee30",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742bb9f2-072c-409d-b043-1344c713056a",
   "metadata": {},
   "source": [
    "### Visualizing\n",
    "\n",
    "After getting a feeling about the size and structure of our data on our hard drive, the next step is to visualize the data.  \n",
    "In the case of image data this can be done by loading random images and plotting them. Also, just looking through them in their folders is a good idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec8aea9-4947-494c-9820-e7041de2c88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "\n",
    "def display_random_img(target_dir, target_class):\n",
    "    target_folder = target_dir + target_class\n",
    "    random_image = random.sample(os.listdir(target_folder), 1)\n",
    "\n",
    "    img = mpimg.imread(target_folder + \"/\" + random_image[0])\n",
    "\n",
    "    plt.imshow(img)\n",
    "    plt.title(target_class)\n",
    "    plt.axis(\"off\");\n",
    "    print(f\"Image shape: {img.shape}\")\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0e3f14",
   "metadata": {},
   "source": [
    "**By running the code cell below we can look through random images and might be able to see out-liners or maybe even wrongly labeled images**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2717cc1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420381f8-3d95-4179-b0bf-f07766464f2e",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Task 02 - Displaying an image\n",
    "\n",
    "##### Use the *display_random_img(target_dir, target_class)* function to display a random image with the corresponding label.\n",
    "\n",
    "If done right, you should see the image below your function call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe73c26-b3eb-42f6-b9c7-158507b11fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a random image\n",
    "img =  # TODO: Call display_random_img(target_dir, target_class) right here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897ac879-7477-4f89-8df8-b21586ade25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display multiple images from the data set\n",
    "plt.figure(figsize=(8, 7))\n",
    "for i in range(9):\n",
    "    plt.subplot(3, 3, i + 1)\n",
    "    img =  # TODO: Call display_random_img(target_dir, target_class) right here (Care: pass the right target class!)\n",
    "    plt.axis(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58296221",
   "metadata": {},
   "source": [
    "**An images representation is nothing else than a array of the pixel values. These values are usually between 0 and 255 and depending on the value the pixel is brighter or darker. By overlaying the 3 color channels red, green, blue (RGB) the colored images we know are created**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949d48d4-a114-4427-bc22-d1fdd728f8af",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: Display image as array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67acf59f",
   "metadata": {},
   "source": [
    "**The image shape is (32, 32, 3) this means the width and the height of the image consists of 32 pixels and 3 color channels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50829b7",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Display image shape (Remember, the image datatype should be a Numpy Array -> https://numpy.org/doc/stable/reference/generated/numpy.shape.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f12a39",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae2bb6d-4069-44cf-98e3-8300c47495e4",
   "metadata": {},
   "source": [
    "### Generate dataset out of CIFAR-10 images\n",
    "\n",
    "Now that we got familiar with our data it is time to create a data set with which we can train our machine learning algorithm.\n",
    "Since the folder structure is split into train and test we will also only generate a training and test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5288d90-9f9a-4115-a444-5a1bb429595d",
   "metadata": {},
   "source": [
    "#### Loading data from the file system and generating training and test sets\n",
    "\n",
    "To load the images from the file system we need to walk through the folders and add the images (normalized) to an array as well as the corresponding labels. Also, for the machine learning algorithms to work with these arrays we need to convert them to numpy arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e355495d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55b6eea-4ea0-4199-a6cd-9ff69c88b355",
   "metadata": {},
   "source": [
    "#### Task 03 - Load images as features and labels  \n",
    "\n",
    "##### Finish the function to load the images and their corresponding labels and return them as numpy arrays\n",
    "* loop through the directory with os.walk https://www.geeksforgeeks.org/os-walk-python/\n",
    "* create a filepath for each file\n",
    "* and load each file with mpimg.imread https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.imread.html\n",
    "* normalize each image https://medium.com/analytics-vidhya/a-tip-a-day-python-tip-8-why-should-we-normalize-image-pixel-values-or-divide-by-255-4608ac5cd26a\n",
    "* do not forget the labels\n",
    "* return them as a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968c2ac9-8dc0-4973-889d-4251821f8e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_as_features_and_labels(target_dir, class_name_dictionary):\n",
    "    \"\"\"\n",
    "    returns \n",
    "    X -- images as numpy array\n",
    "    y -- labels as numpy array\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Add your code here\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b806b287",
   "metadata": {},
   "source": [
    "With help of the function above we can now load the images and there labels as a training and test set (this could take a while, depending on your hardware!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023a3fa8-a3aa-4e67-83cc-123420d5120c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, y_train = load_images_as_features_and_labels('data/CIFAR-10-images/train/', class_names_dict)\n",
    "X_test, y_test = load_images_as_features_and_labels('data/CIFAR-10-images/test/', class_names_dict)\n",
    "\n",
    "X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5029cb",
   "metadata": {},
   "source": [
    "### Visualizing once again\n",
    "\n",
    "After loading our data as training and test set it is once again important to visualize the loaded data. This way our understanding of our data grows, and we might notice errors we would not otherwise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4967c0cb",
   "metadata": {},
   "source": [
    "First investigate the shape of our training and test set. One of the most common errors in ML is that the input shape for the algorithm is not compatible with the shape of our data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d06ec60-f568-4320-9fbd-422675577509",
   "metadata": {},
   "source": [
    "TODO: Print the shape of each data set. It should look like this `((50000, 32, 32, 3), (50000,), (10000, 32, 32, 3), (10000,))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7112c9c4-3518-4e69-8c33-5f106dcf0424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e630f6a2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3970de45-510d-464d-ba2c-09e96605b1c4",
   "metadata": {},
   "source": [
    "These shapes tell us the following:\n",
    "* X_train is a four dimensional array\n",
    "    - The first dimension tells us how many samples are in the array - 50000\n",
    "    - The second dimension tells us how many pixel values the image has to represent its height - 32\n",
    "    - The third dimension tells us how many pixel values the image has to represent its width - 32\n",
    "    - The fourth dimension tells us how many color channels the image has (red ,green blue - RGB) - 3\n",
    "* y_train is a one dimensional array containing the corresponding labels to X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88ec408",
   "metadata": {},
   "source": [
    "Next we will visualize the actual images contained in our loaded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554a0699-2a44-411c-914f-acd53f6fabf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_multiple_figures(images_array):\n",
    "    plt.figure(figsize=(9, 9))\n",
    "    for idx, img in enumerate(images_array[:100]):\n",
    "        plt.subplot(10, 10, idx + 1)\n",
    "        plt.imshow(img, cmap=\"binary\")\n",
    "        plt.axis(\"off\")\n",
    "    plt.subplots_adjust(wspace=0, hspace=0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0641187f-624f-469a-b067-dd9b43abd417",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_multiple_figures(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0d3ed7-3b85-479a-a012-b8d8ca2cbd89",
   "metadata": {},
   "source": [
    "**By visualizing the train data we can see that the training set contains the images as we have loaded it. Meaning the data is sorted in order of the walked through folders**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aea33d3",
   "metadata": {},
   "source": [
    "## Training of our machine learning algorithm\n",
    "\n",
    "We will use the random forest classifier algorithm to try and classify our test images.\n",
    "For this we first need to once again prepare our data for the algorithm and then fit (train) it on our training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d835ac17",
   "metadata": {},
   "source": [
    "**Random forest classifier needs input in shape of (batch, flattened image) i.e (50000, 3072) -> 32*32*3  = 3072. This means we have to reshape or data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40401ba",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b348ac-c5df-4efe-93d6-aa7a84c7c591",
   "metadata": {},
   "source": [
    "#### TASK 04 - Reshaping your data set to make it compatible with the ML algorithm\n",
    "##### Use numpys shape and reshape functions to reshape the data https://numpy.org/doc/stable/reference/generated/numpy.reshape.html\n",
    "\n",
    "It should look like this `((50000, 3072), (10000, 3072))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619faea8-a1f1-450f-b914-12c0a30863b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Random forest classifier needs input in shape of (batch, flattended image) i.e (50000, 3072) -> 32*32*3  = 3072\n",
    "# ADD your code here\n",
    "\n",
    "X_train_flattened.shape, X_test_flattened.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b2b547",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa22c4f",
   "metadata": {},
   "source": [
    "After our data has the right shape we can fit/train a RandomForest Classifier on it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8f2c96",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff19f3cd-fb71-4342-9893-91acf7c21163",
   "metadata": {},
   "source": [
    "#### TASK 05 - load and train a RandomForestClassifier\n",
    "##### use scikit-learns random forest classifier and use your data set to train it https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4415dc82-12c3-4775-a89c-b3dbd591245a",
   "metadata": {},
   "outputs": [],
   "source": [
    "% % time\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rnf_clf =  #TODO: load the classifier\n",
    "# TODO: train the classifier on the data set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb02caa-d59f-4c18-a488-e20bcd5d2ff3",
   "metadata": {},
   "source": [
    "With the classifier now trained on the training data we can make predictions with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f57932",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "y_pred_clf =  # TODO: Use the trained classifier to make predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb0483c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07617c7e-e362-4b67-a993-e389661fbad1",
   "metadata": {},
   "source": [
    "### Evaluation of the classifier\n",
    "\n",
    "It is always important test our classifier. After all how should we know how well it performs?\n",
    "To measure its performance multiple metrics can be used. The most important being:\n",
    "* accuracy\n",
    "* precision\n",
    "* recall\n",
    "* f1  \n",
    "\n",
    "We can calculate it ourselves or import a function from scikit-learn to do it for us.\n",
    "Now with our predictions and the test data we can evaluate the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f7f42f-0f85-4cd0-b1d4-f10baf05d0f8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44277993-5286-44c4-ae84-a201765e2345",
   "metadata": {},
   "source": [
    "#### TASK 06 - Evaluate the classifier\n",
    "##### Use the scikit learns score functions to evaluate accuracy, precision, recall and the f1 score and print the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd9d730",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# TODO: Add your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371492ea-fda4-4e3f-a198-5d04c27cc62c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a15004d-b84e-4ee7-b1f8-385bc0bd6bb8",
   "metadata": {},
   "source": [
    "### Viszalization of the evaluation\n",
    "\n",
    "Another way to evaluate our classifier is by visualizing its performance.  \n",
    "One way to do that is to use a confusion matrix.  \n",
    "This matrix tells us how many samples were classified correctly and how many were falsely classified as a different class.\n",
    "https://en.wikipedia.org/wiki/Confusion_matrix  \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9d8e1c-26fb-482c-9afb-bb1c37749446",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_clf)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names_dict.keys())\n",
    "disp.plot(xticks_rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb65ad80-41d8-48c5-85f5-87d0bcf2555d",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4b15f5-ab6d-47d4-a131-64b657ff6074",
   "metadata": {},
   "source": [
    "## Part 2 - Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d57108-00f4-4652-ab19-328d9386a7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tf.__version__  # IMPORTANT: This should print '2.8.0', if your version is newer, you might not be able to use your GPU for training!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88b3316-5cd6-43bc-bb6b-3838dc71e1f8",
   "metadata": {},
   "source": [
    "If you have installed tensorflow with gpu support you should first of all be able to output your GPU.  \n",
    "To install tensorflow with GPU support please follow the steps described here:  \n",
    "https://www.tensorflow.org/install/pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172269e7-0566-4124-91ec-d1730bfe6f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we're using a GPU\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d112541-ec66-43d8-b9f7-4fd61cd5baa7",
   "metadata": {},
   "source": [
    "Next to make sure tensorflow is able to detect your GPU get the device with `tf.config.list_physical_devices('GPU')`.  \n",
    "And enable mixed precision if your GPU is good enough.\n",
    "For more information about mixed precision look here:\n",
    "https://www.tensorflow.org/guide/mixed_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842374ca-c493-4c0e-92e2-12db1e2c3779",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import mixed_precision\n",
    "\n",
    "gpu_devices = tf.config.list_physical_devices('GPU')\n",
    "if gpu_devices:\n",
    "    details = tf.config.experimental.get_device_details(gpu_devices[0])\n",
    "    print(f\"Your GPUs compute capability is {details['compute_capability'][:]}\")\n",
    "    if details['compute_capability'][0] >= 7.0:\n",
    "        mixed_precision.set_global_policy('mixed_float16')\n",
    "        print(f\"Your global policy has been set to: {mixed_precision.global_policy()}\")\n",
    "    else:\n",
    "        print(\n",
    "            f\"Your GPU does not support mixed precision, the global policy is left at: {mixed_precision.global_policy()}\")\n",
    "else:\n",
    "    print(\"No GPU device could be found on your machine\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7428ee55-ed1f-4157-b4f9-99a8ab44fcea",
   "metadata": {},
   "source": [
    "### Load images from directory with image_dataset_from_directory & create augmentation layer\n",
    "\n",
    "This step describes how data can be loaded and how a CNN model can be trained.  \n",
    "For more information and examples look here:  \n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/utils/image_dataset_from_directory  \n",
    "https://www.tensorflow.org/api_docs/python/tf/data/Dataset  \n",
    "https://www.tensorflow.org/tutorials/load_data/images  \n",
    "https://www.tensorflow.org/tutorials/images/data_augmentation  \n",
    "https://www.tensorflow.org/guide/keras/preprocessing_layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be058a72",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab714b8-828e-47d3-b356-aef79fe57372",
   "metadata": {},
   "source": [
    "#### TASK 07 - Load your datasets for training and testing the DL model\n",
    "##### Use the Tensorflow 'image_dataset_from_directory' method! (use shuffle = True on train_data!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1439a139-a499-48f6-b303-a826e9f6ae71",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"data/CIFAR-10-images/train/\"\n",
    "test_dir = \"data/CIFAR-10-images/test/\"\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_data =  # TODO load the training set\n",
    "\n",
    "test_data =  # TODO load the testing set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7766c9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82704ee-9b5c-433a-bc97-7b462e3c1bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the amount of classes we have - should be 10\n",
    "num_classes = len(train_data.class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6ba322-a334-4dfe-b5a9-5f4b849fcd0a",
   "metadata": {},
   "source": [
    "### CNN Architecture\n",
    "\n",
    "* Sequential API - https://www.tensorflow.org/api_docs/python/tf/keras/Sequential\n",
    "* Functional API - https://www.tensorflow.org/guide/keras/functional\n",
    "* Subclassing - https://www.tensorflow.org/guide/keras/custom_layers_and_models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1ebb66-2112-4ce5-ba51-c7f6b8f400df",
   "metadata": {},
   "source": [
    "To use data augmentation inside our CNN model we first need to create a data augmentation \"layer\".  \n",
    "For this we use the Sequential API and stack different preprocessing layers.  \n",
    "The different layers and their options can be found in the documentation:  \n",
    "https://www.tensorflow.org/guide/keras/preprocessing_layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27127f1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9753a8b-34ca-4268-b0fb-b08ded7dd80e",
   "metadata": {},
   "source": [
    "#### Task 08 - Build a sequential model with preprocessing layers\n",
    "##### Use keras sequential API ( https://www.tensorflow.org/api_docs/python/tf/keras/Sequential) to stack preprocessing layers. Do not forget to normalize/rescale your data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cdb306-09e9-4352-b3ff-b1634b9615ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a sequential model which will be added as a layer in the model\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "\n",
    "data_augmentation =  #TODO: Build your sequential model here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8decf8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262a1755-9335-4247-986d-2786cf25c38a",
   "metadata": {},
   "source": [
    "Next we build our CNN model, this time with help of the functional API.  \n",
    "It is the same model as before, but with addition of our `data_augmentation` layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197217bf",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cf4e12-0653-4346-8997-0ca692e5172a",
   "metadata": {},
   "source": [
    "#### Task 09 - Build a model with the functional API\n",
    "##### Use keras functional API (https://www.tensorflow.org/guide/keras/functional) to build your CNN do not forget to include your data_augmentation layer\n",
    "You can use layers like: \n",
    "* Conv2D, \n",
    "* Dense,\n",
    "* BatchNormalization, \n",
    "* MaxPooling,\n",
    "* Dropout,\n",
    "* Flatten (for input into Dense)\n",
    "* ....\n",
    "\n",
    "<p><strong style=\"color:red\">Attention: </strong>Experiment with different layers and number of layers until you reach at least 70% accuracy</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54dc37d8-90b7-4ba1-bd75-7d79a1f2678e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model with functional api\n",
    "\n",
    "inputs = tf.keras.layers.Input(shape=(32, 32, 3), name='input_layer')\n",
    "x =  # TODO: add your augmentation layer/model after the input layer\n",
    "#TODO: add your layers\n",
    "\n",
    "\n",
    "model =  # TODO: use keras.Model() to build your model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01c13e9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a25273b-976f-422e-8add-b4e480e7398d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b39f11-b3cf-42be-b14a-4994e0c888ca",
   "metadata": {},
   "source": [
    "The next steps of compiling, training and evaluating the model are the same as before.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30e0be9-f6aa-499b-a649-8c37834aa751",
   "metadata": {},
   "source": [
    "To train our model we first need to compile it. In this step the most important arguments are the: \n",
    "* loss function - https://www.tensorflow.org/api_docs/python/tf/keras/losses\n",
    "* optimizer - https://www.tensorflow.org/api_docs/python/tf/keras/optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6a0526",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34799c5c-4f59-48ba-a2bc-5242f79aafd2",
   "metadata": {},
   "source": [
    "#### Task 10 - Compile and train your model\n",
    "##### use the compile and fit methods respectively to compile and fit/train your model\n",
    "\n",
    "For compiling use:\n",
    "* CategoricalCrossentropy as a loss function\n",
    "* Adam as a optimizer\n",
    "* and track at least accuracy, Recall, and Precision during training\n",
    "\n",
    "For fitting use:\n",
    "* your training data set\n",
    "* your test data set for validation during training\n",
    "* Experiment with the number of epochs to avoid overfitting or underfitting your data (https://www.ibm.com/cloud/learn/overfitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ac5e76-e571-4ecc-8285-cacb8fa1167f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: compile your model here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28931e1-d008-4fc6-a95d-230741b9a157",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "history =  # TODO: fit your model here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d918a0c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9484c552-4c53-4303-85a2-acc174fdfa67",
   "metadata": {},
   "source": [
    "### Evaluate the model\n",
    "\n",
    "After training we can evaluate our models performance on the test data.  \n",
    "for this we can use the `evaluate` method.  \n",
    "This will evaluate the model on the metrics we decided to track during training (loss is always tracked)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2962bb12-40a5-44dd-a54e-63311ce4d769",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy, recall, precision = model.evaluate(test_data)\n",
    "print(\"Loss:\", loss)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"Precision:\", precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf213fd0-f44a-412a-ac1b-90535f7f7070",
   "metadata": {},
   "source": [
    "Another important step in evaluating our model is to visualize how the various metrics  behaved during training.  \n",
    "This way unusual patterns like overfitting and underfitting can be discovered.  \n",
    "To plot these curves the history object which is created during training can be used.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f00202c-a467-4cae-9130-79172709574a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the validation and training data separately\n",
    "def plot_loss_curves(history):\n",
    "    \"\"\"\n",
    "    Returns separate loss curves for training and validation metrics.\n",
    "    \"\"\"\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    accuracy = history.history['accuracy']\n",
    "    val_accuracy = history.history['val_accuracy']\n",
    "\n",
    "    recall = history.history['recall']\n",
    "    val_recall = history.history['val_recall']\n",
    "\n",
    "    precision = history.history['precision']\n",
    "    val_precision = history.history['val_precision']\n",
    "\n",
    "    epochs = range(len(history.history['loss']))\n",
    "\n",
    "    # Plot loss\n",
    "    plt.plot(epochs, loss, label='training_loss')\n",
    "    plt.plot(epochs, val_loss, label='val_loss')\n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot accuracy\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, accuracy, label='training_accuracy')\n",
    "    plt.plot(epochs, val_accuracy, label='val_accuracy')\n",
    "    plt.title('Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend();\n",
    "\n",
    "    # Plot recall\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, recall, label='training_recall')\n",
    "    plt.plot(epochs, val_recall, label='val_recall')\n",
    "    plt.title('Recall')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend();\n",
    "\n",
    "    # Plot precision\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, precision, label='training_precision')\n",
    "    plt.plot(epochs, val_precision, label='val_precision')\n",
    "    plt.title('Precision')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14aec116-3710-40ed-896d-ba550ef1814a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out the loss curves\n",
    "plot_loss_curves(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bdfdc5-7e46-4c6a-9d81-8374c2380271",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f639476c-465b-41cb-821a-c2b6a044d4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.concatenate([y for x, y in test_data], axis=0)\n",
    "y_test.argmax(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba9ccc6-368a-4291-839c-530b913262a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred.argmax(axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d4d055-0575-42ac-bf5c-1b7b0097cb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "cm = confusion_matrix(y_test.argmax(axis=1), y_pred.argmax(axis=1))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names_dict.keys())\n",
    "disp.plot(xticks_rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2620197a-5885-4e63-8ddc-916b18880b8b",
   "metadata": {},
   "source": [
    "## Save the model for later use (i.e. deploying it to production, making predictions, grad-cam, etc)\n",
    "\n",
    "After we have trained a model we can easily save it with `model.save()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807a485a-7be5-4921-98c6-c0f2c21ff190",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('saved_models/model_with_data_augmentation_layer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838aad44-ffe6-4d1b-b703-57afb4b0b851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and we can load it easily with the load_model method\n",
    "loaded_model = tf.keras.models.load_model('saved_models/model_with_data_augmentation_layer/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ef62ce-2887-4b17-a95b-53d2036e6cea",
   "metadata": {},
   "source": [
    "## Good Luck (or well done!)\n",
    "\n",
    "If you just read through everything, good luck with this lab! Otherwise, well done and please send it to your instructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09d20fc-8fad-453c-969f-9f04d072a10e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
